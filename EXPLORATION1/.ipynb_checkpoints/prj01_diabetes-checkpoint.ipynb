{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5517fb0d",
   "metadata": {},
   "source": [
    "## (1) 데이터 가져오기\n",
    "sklearn.datasets 의 load_diabetes 에서 데이터를 가져와주세요.\n",
    "diabetes 의 data 를 df_X 에, target 을 df_y 에 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc13ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "dataset = sklearn.datasets.load_diabetes()\n",
    "\n",
    "df_x=dataset.data\n",
    "df_y=dataset.target\n",
    "\n",
    "print(df_x.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358eb23",
   "metadata": {},
   "source": [
    "## (2) 모델에 입력할 데이터 X 준비하기\n",
    "df_X 에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc99c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_X = np.array(df_x)\n",
    "print(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1520535",
   "metadata": {},
   "source": [
    "## (3) 모델에 예측할 데이터 y 준비하기\n",
    "df_y 에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615c7000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "df_y = np.array(df_y)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45295b07",
   "metadata": {},
   "source": [
    "## (4) train 데이터와 test 데이터로 분리하기\n",
    "X 와 y  데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b1c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf79e2",
   "metadata": {},
   "source": [
    "## (5) 모델 준비하기\n",
    "입력 데이터 개수에 맞는 가중치 W 와 b 를 준비해주세요.\n",
    "모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47684ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49435fc6",
   "metadata": {},
   "source": [
    "## (6) 손실함수 loss 정의하기\n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275c096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "  mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "  return mse\n",
    "\n",
    "def loss(x, w, b, y):\n",
    "  predictions = model(x, w, b)\n",
    "  L = MSE(predictions, y)\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7000a",
   "metadata": {},
   "source": [
    "## (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "기울기를 계산하는 gradient  함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d478a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2953023",
   "metadata": {},
   "source": [
    "## (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "학습률, learning rate 를 설정해주세요\n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c6cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec7648",
   "metadata": {},
   "source": [
    "## (9) 모델 학습하기\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "입력하는 데이터인 X 에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfdcbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_ = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68764e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 6322.0446\n",
      "Iteration 20 : Loss 5976.6911\n",
      "Iteration 30 : Loss 5899.7231\n",
      "Iteration 40 : Loss 5828.1176\n",
      "Iteration 50 : Loss 5758.7652\n",
      "Iteration 60 : Loss 5691.5552\n",
      "Iteration 70 : Loss 5626.4145\n",
      "Iteration 80 : Loss 5563.2724\n",
      "Iteration 90 : Loss 5502.0611\n",
      "Iteration 100 : Loss 5442.7151\n",
      "Iteration 110 : Loss 5385.1712\n",
      "Iteration 120 : Loss 5329.3682\n",
      "Iteration 130 : Loss 5275.2474\n",
      "Iteration 140 : Loss 5222.7519\n",
      "Iteration 150 : Loss 5171.8270\n",
      "Iteration 160 : Loss 5122.4197\n",
      "Iteration 170 : Loss 5074.4789\n",
      "Iteration 180 : Loss 5027.9556\n",
      "Iteration 190 : Loss 4982.8021\n",
      "Iteration 200 : Loss 4938.9725\n",
      "Iteration 210 : Loss 4896.4226\n",
      "Iteration 220 : Loss 4855.1098\n",
      "Iteration 230 : Loss 4814.9928\n",
      "Iteration 240 : Loss 4776.0317\n",
      "Iteration 250 : Loss 4738.1883\n",
      "Iteration 260 : Loss 4701.4255\n",
      "Iteration 270 : Loss 4665.7075\n",
      "Iteration 280 : Loss 4630.9998\n",
      "Iteration 290 : Loss 4597.2691\n",
      "Iteration 300 : Loss 4564.4833\n",
      "Iteration 310 : Loss 4532.6113\n",
      "Iteration 320 : Loss 4501.6231\n",
      "Iteration 330 : Loss 4471.4899\n",
      "Iteration 340 : Loss 4442.1838\n",
      "Iteration 350 : Loss 4413.6778\n",
      "Iteration 360 : Loss 4385.9459\n",
      "Iteration 370 : Loss 4358.9630\n",
      "Iteration 380 : Loss 4332.7049\n",
      "Iteration 390 : Loss 4307.1482\n",
      "Iteration 400 : Loss 4282.2702\n",
      "Iteration 410 : Loss 4258.0492\n",
      "Iteration 420 : Loss 4234.4641\n",
      "Iteration 430 : Loss 4211.4946\n",
      "Iteration 440 : Loss 4189.1211\n",
      "Iteration 450 : Loss 4167.3246\n",
      "Iteration 460 : Loss 4146.0867\n",
      "Iteration 470 : Loss 4125.3899\n",
      "Iteration 480 : Loss 4105.2170\n",
      "Iteration 490 : Loss 4085.5516\n",
      "Iteration 500 : Loss 4066.3778\n",
      "Iteration 510 : Loss 4047.6802\n",
      "Iteration 520 : Loss 4029.4438\n",
      "Iteration 530 : Loss 4011.6545\n",
      "Iteration 540 : Loss 3994.2983\n",
      "Iteration 550 : Loss 3977.3619\n",
      "Iteration 560 : Loss 3960.8323\n",
      "Iteration 570 : Loss 3944.6971\n",
      "Iteration 580 : Loss 3928.9441\n",
      "Iteration 590 : Loss 3913.5618\n",
      "Iteration 600 : Loss 3898.5389\n",
      "Iteration 610 : Loss 3883.8645\n",
      "Iteration 620 : Loss 3869.5280\n",
      "Iteration 630 : Loss 3855.5194\n",
      "Iteration 640 : Loss 3841.8288\n",
      "Iteration 650 : Loss 3828.4468\n",
      "Iteration 660 : Loss 3815.3642\n",
      "Iteration 670 : Loss 3802.5720\n",
      "Iteration 680 : Loss 3790.0618\n",
      "Iteration 690 : Loss 3777.8253\n",
      "Iteration 700 : Loss 3765.8545\n",
      "Iteration 710 : Loss 3754.1417\n",
      "Iteration 720 : Loss 3742.6793\n",
      "Iteration 730 : Loss 3731.4602\n",
      "Iteration 740 : Loss 3720.4773\n",
      "Iteration 750 : Loss 3709.7240\n",
      "Iteration 760 : Loss 3699.1936\n",
      "Iteration 770 : Loss 3688.8799\n",
      "Iteration 780 : Loss 3678.7768\n",
      "Iteration 790 : Loss 3668.8783\n",
      "Iteration 800 : Loss 3659.1788\n",
      "Iteration 810 : Loss 3649.6726\n",
      "Iteration 820 : Loss 3640.3545\n",
      "Iteration 830 : Loss 3631.2194\n",
      "Iteration 840 : Loss 3622.2621\n",
      "Iteration 850 : Loss 3613.4778\n",
      "Iteration 860 : Loss 3604.8619\n",
      "Iteration 870 : Loss 3596.4098\n",
      "Iteration 880 : Loss 3588.1172\n",
      "Iteration 890 : Loss 3579.9798\n",
      "Iteration 900 : Loss 3571.9934\n",
      "Iteration 910 : Loss 3564.1542\n",
      "Iteration 920 : Loss 3556.4582\n",
      "Iteration 930 : Loss 3548.9017\n",
      "Iteration 940 : Loss 3541.4811\n",
      "Iteration 950 : Loss 3534.1930\n",
      "Iteration 960 : Loss 3527.0339\n",
      "Iteration 970 : Loss 3520.0006\n",
      "Iteration 980 : Loss 3513.0898\n",
      "Iteration 990 : Loss 3506.2985\n",
      "Iteration 1000 : Loss 3499.6238\n",
      "Iteration 1010 : Loss 3493.0627\n",
      "Iteration 1020 : Loss 3486.6124\n",
      "Iteration 1030 : Loss 3480.2703\n",
      "Iteration 1040 : Loss 3474.0336\n",
      "Iteration 1050 : Loss 3467.8999\n",
      "Iteration 1060 : Loss 3461.8667\n",
      "Iteration 1070 : Loss 3455.9316\n",
      "Iteration 1080 : Loss 3450.0923\n",
      "Iteration 1090 : Loss 3444.3464\n",
      "Iteration 1100 : Loss 3438.6919\n",
      "Iteration 1110 : Loss 3433.1266\n",
      "Iteration 1120 : Loss 3427.6485\n",
      "Iteration 1130 : Loss 3422.2556\n",
      "Iteration 1140 : Loss 3416.9459\n",
      "Iteration 1150 : Loss 3411.7176\n",
      "Iteration 1160 : Loss 3406.5688\n",
      "Iteration 1170 : Loss 3401.4978\n",
      "Iteration 1180 : Loss 3396.5029\n",
      "Iteration 1190 : Loss 3391.5823\n",
      "Iteration 1200 : Loss 3386.7346\n",
      "Iteration 1210 : Loss 3381.9581\n",
      "Iteration 1220 : Loss 3377.2512\n",
      "Iteration 1230 : Loss 3372.6126\n",
      "Iteration 1240 : Loss 3368.0408\n",
      "Iteration 1250 : Loss 3363.5343\n",
      "Iteration 1260 : Loss 3359.0919\n",
      "Iteration 1270 : Loss 3354.7121\n",
      "Iteration 1280 : Loss 3350.3938\n",
      "Iteration 1290 : Loss 3346.1357\n",
      "Iteration 1300 : Loss 3341.9365\n",
      "Iteration 1310 : Loss 3337.7951\n",
      "Iteration 1320 : Loss 3333.7103\n",
      "Iteration 1330 : Loss 3329.6811\n",
      "Iteration 1340 : Loss 3325.7063\n",
      "Iteration 1350 : Loss 3321.7848\n",
      "Iteration 1360 : Loss 3317.9158\n",
      "Iteration 1370 : Loss 3314.0980\n",
      "Iteration 1380 : Loss 3310.3307\n",
      "Iteration 1390 : Loss 3306.6128\n",
      "Iteration 1400 : Loss 3302.9434\n",
      "Iteration 1410 : Loss 3299.3217\n",
      "Iteration 1420 : Loss 3295.7466\n",
      "Iteration 1430 : Loss 3292.2175\n",
      "Iteration 1440 : Loss 3288.7334\n",
      "Iteration 1450 : Loss 3285.2936\n",
      "Iteration 1460 : Loss 3281.8973\n",
      "Iteration 1470 : Loss 3278.5436\n",
      "Iteration 1480 : Loss 3275.2319\n",
      "Iteration 1490 : Loss 3271.9614\n",
      "Iteration 1500 : Loss 3268.7314\n",
      "Iteration 1510 : Loss 3265.5412\n",
      "Iteration 1520 : Loss 3262.3901\n",
      "Iteration 1530 : Loss 3259.2776\n",
      "Iteration 1540 : Loss 3256.2028\n",
      "Iteration 1550 : Loss 3253.1652\n",
      "Iteration 1560 : Loss 3250.1643\n",
      "Iteration 1570 : Loss 3247.1992\n",
      "Iteration 1580 : Loss 3244.2696\n",
      "Iteration 1590 : Loss 3241.3749\n",
      "Iteration 1600 : Loss 3238.5143\n",
      "Iteration 1610 : Loss 3235.6875\n",
      "Iteration 1620 : Loss 3232.8939\n",
      "Iteration 1630 : Loss 3230.1329\n",
      "Iteration 1640 : Loss 3227.4041\n",
      "Iteration 1650 : Loss 3224.7070\n",
      "Iteration 1660 : Loss 3222.0410\n",
      "Iteration 1670 : Loss 3219.4057\n",
      "Iteration 1680 : Loss 3216.8006\n",
      "Iteration 1690 : Loss 3214.2253\n",
      "Iteration 1700 : Loss 3211.6794\n",
      "Iteration 1710 : Loss 3209.1623\n",
      "Iteration 1720 : Loss 3206.6736\n",
      "Iteration 1730 : Loss 3204.2131\n",
      "Iteration 1740 : Loss 3201.7801\n",
      "Iteration 1750 : Loss 3199.3744\n",
      "Iteration 1760 : Loss 3196.9955\n",
      "Iteration 1770 : Loss 3194.6431\n",
      "Iteration 1780 : Loss 3192.3167\n",
      "Iteration 1790 : Loss 3190.0161\n",
      "Iteration 1800 : Loss 3187.7408\n",
      "Iteration 1810 : Loss 3185.4905\n",
      "Iteration 1820 : Loss 3183.2648\n",
      "Iteration 1830 : Loss 3181.0635\n",
      "Iteration 1840 : Loss 3178.8861\n",
      "Iteration 1850 : Loss 3176.7323\n",
      "Iteration 1860 : Loss 3174.6019\n",
      "Iteration 1870 : Loss 3172.4945\n",
      "Iteration 1880 : Loss 3170.4098\n",
      "Iteration 1890 : Loss 3168.3475\n",
      "Iteration 1900 : Loss 3166.3073\n",
      "Iteration 1910 : Loss 3164.2889\n",
      "Iteration 1920 : Loss 3162.2920\n",
      "Iteration 1930 : Loss 3160.3164\n",
      "Iteration 1940 : Loss 3158.3617\n",
      "Iteration 1950 : Loss 3156.4277\n",
      "Iteration 1960 : Loss 3154.5142\n",
      "Iteration 1970 : Loss 3152.6207\n",
      "Iteration 1980 : Loss 3150.7472\n",
      "Iteration 1990 : Loss 3148.8934\n",
      "Iteration 2000 : Loss 3147.0590\n",
      "Iteration 2010 : Loss 3145.2437\n",
      "Iteration 2020 : Loss 3143.4473\n",
      "Iteration 2030 : Loss 3141.6696\n",
      "Iteration 2040 : Loss 3139.9104\n",
      "Iteration 2050 : Loss 3138.1694\n",
      "Iteration 2060 : Loss 3136.4464\n",
      "Iteration 2070 : Loss 3134.7412\n",
      "Iteration 2080 : Loss 3133.0535\n",
      "Iteration 2090 : Loss 3131.3832\n",
      "Iteration 2100 : Loss 3129.7300\n",
      "Iteration 2110 : Loss 3128.0938\n",
      "Iteration 2120 : Loss 3126.4743\n",
      "Iteration 2130 : Loss 3124.8713\n",
      "Iteration 2140 : Loss 3123.2846\n",
      "Iteration 2150 : Loss 3121.7141\n",
      "Iteration 2160 : Loss 3120.1595\n",
      "Iteration 2170 : Loss 3118.6207\n",
      "Iteration 2180 : Loss 3117.0975\n",
      "Iteration 2190 : Loss 3115.5896\n",
      "Iteration 2200 : Loss 3114.0970\n",
      "Iteration 2210 : Loss 3112.6194\n",
      "Iteration 2220 : Loss 3111.1567\n",
      "Iteration 2230 : Loss 3109.7086\n",
      "Iteration 2240 : Loss 3108.2751\n",
      "Iteration 2250 : Loss 3106.8559\n",
      "Iteration 2260 : Loss 3105.4509\n",
      "Iteration 2270 : Loss 3104.0599\n",
      "Iteration 2280 : Loss 3102.6828\n",
      "Iteration 2290 : Loss 3101.3194\n",
      "Iteration 2300 : Loss 3099.9696\n",
      "Iteration 2310 : Loss 3098.6331\n",
      "Iteration 2320 : Loss 3097.3100\n",
      "Iteration 2330 : Loss 3095.9999\n",
      "Iteration 2340 : Loss 3094.7027\n",
      "Iteration 2350 : Loss 3093.4184\n",
      "Iteration 2360 : Loss 3092.1468\n",
      "Iteration 2370 : Loss 3090.8876\n",
      "Iteration 2380 : Loss 3089.6409\n",
      "Iteration 2390 : Loss 3088.4064\n",
      "Iteration 2400 : Loss 3087.1841\n",
      "Iteration 2410 : Loss 3085.9737\n",
      "Iteration 2420 : Loss 3084.7752\n",
      "Iteration 2430 : Loss 3083.5884\n",
      "Iteration 2440 : Loss 3082.4132\n",
      "Iteration 2450 : Loss 3081.2494\n",
      "Iteration 2460 : Loss 3080.0970\n",
      "Iteration 2470 : Loss 3078.9559\n",
      "Iteration 2480 : Loss 3077.8258\n",
      "Iteration 2490 : Loss 3076.7067\n",
      "Iteration 2500 : Loss 3075.5985\n",
      "Iteration 2510 : Loss 3074.5010\n",
      "Iteration 2520 : Loss 3073.4141\n",
      "Iteration 2530 : Loss 3072.3378\n",
      "Iteration 2540 : Loss 3071.2719\n",
      "Iteration 2550 : Loss 3070.2162\n",
      "Iteration 2560 : Loss 3069.1707\n",
      "Iteration 2570 : Loss 3068.1354\n",
      "Iteration 2580 : Loss 3067.1099\n",
      "Iteration 2590 : Loss 3066.0944\n",
      "Iteration 2600 : Loss 3065.0886\n",
      "Iteration 2610 : Loss 3064.0924\n",
      "Iteration 2620 : Loss 3063.1058\n",
      "Iteration 2630 : Loss 3062.1287\n",
      "Iteration 2640 : Loss 3061.1609\n",
      "Iteration 2650 : Loss 3060.2024\n",
      "Iteration 2660 : Loss 3059.2530\n",
      "Iteration 2670 : Loss 3058.3127\n",
      "Iteration 2680 : Loss 3057.3813\n",
      "Iteration 2690 : Loss 3056.4589\n",
      "Iteration 2700 : Loss 3055.5452\n",
      "Iteration 2710 : Loss 3054.6402\n",
      "Iteration 2720 : Loss 3053.7438\n",
      "Iteration 2730 : Loss 3052.8559\n",
      "Iteration 2740 : Loss 3051.9764\n",
      "Iteration 2750 : Loss 3051.1053\n",
      "Iteration 2760 : Loss 3050.2424\n",
      "Iteration 2770 : Loss 3049.3877\n",
      "Iteration 2780 : Loss 3048.5411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2790 : Loss 3047.7024\n",
      "Iteration 2800 : Loss 3046.8717\n",
      "Iteration 2810 : Loss 3046.0488\n",
      "Iteration 2820 : Loss 3045.2337\n",
      "Iteration 2830 : Loss 3044.4262\n",
      "Iteration 2840 : Loss 3043.6264\n",
      "Iteration 2850 : Loss 3042.8340\n",
      "Iteration 2860 : Loss 3042.0491\n",
      "Iteration 2870 : Loss 3041.2716\n",
      "Iteration 2880 : Loss 3040.5014\n",
      "Iteration 2890 : Loss 3039.7383\n",
      "Iteration 2900 : Loss 3038.9824\n",
      "Iteration 2910 : Loss 3038.2336\n",
      "Iteration 2920 : Loss 3037.4918\n",
      "Iteration 2930 : Loss 3036.7569\n",
      "Iteration 2940 : Loss 3036.0289\n",
      "Iteration 2950 : Loss 3035.3077\n",
      "Iteration 2960 : Loss 3034.5932\n",
      "Iteration 2970 : Loss 3033.8853\n",
      "Iteration 2980 : Loss 3033.1840\n",
      "Iteration 2990 : Loss 3032.4893\n",
      "Iteration 3000 : Loss 3031.8010\n",
      "Iteration 3010 : Loss 3031.1191\n",
      "Iteration 3020 : Loss 3030.4435\n",
      "Iteration 3030 : Loss 3029.7742\n",
      "Iteration 3040 : Loss 3029.1111\n",
      "Iteration 3050 : Loss 3028.4541\n",
      "Iteration 3060 : Loss 3027.8032\n",
      "Iteration 3070 : Loss 3027.1583\n",
      "Iteration 3080 : Loss 3026.5194\n",
      "Iteration 3090 : Loss 3025.8863\n",
      "Iteration 3100 : Loss 3025.2592\n",
      "Iteration 3110 : Loss 3024.6377\n",
      "Iteration 3120 : Loss 3024.0220\n",
      "Iteration 3130 : Loss 3023.4120\n",
      "Iteration 3140 : Loss 3022.8076\n",
      "Iteration 3150 : Loss 3022.2087\n",
      "Iteration 3160 : Loss 3021.6154\n",
      "Iteration 3170 : Loss 3021.0275\n",
      "Iteration 3180 : Loss 3020.4449\n",
      "Iteration 3190 : Loss 3019.8677\n",
      "Iteration 3200 : Loss 3019.2958\n",
      "Iteration 3210 : Loss 3018.7292\n",
      "Iteration 3220 : Loss 3018.1677\n",
      "Iteration 3230 : Loss 3017.6113\n",
      "Iteration 3240 : Loss 3017.0600\n",
      "Iteration 3250 : Loss 3016.5138\n",
      "Iteration 3260 : Loss 3015.9725\n",
      "Iteration 3270 : Loss 3015.4361\n",
      "Iteration 3280 : Loss 3014.9047\n",
      "Iteration 3290 : Loss 3014.3781\n",
      "Iteration 3300 : Loss 3013.8562\n",
      "Iteration 3310 : Loss 3013.3391\n",
      "Iteration 3320 : Loss 3012.8267\n",
      "Iteration 3330 : Loss 3012.3190\n",
      "Iteration 3340 : Loss 3011.8158\n",
      "Iteration 3350 : Loss 3011.3172\n",
      "Iteration 3360 : Loss 3010.8231\n",
      "Iteration 3370 : Loss 3010.3335\n",
      "Iteration 3380 : Loss 3009.8484\n",
      "Iteration 3390 : Loss 3009.3676\n",
      "Iteration 3400 : Loss 3008.8911\n",
      "Iteration 3410 : Loss 3008.4189\n",
      "Iteration 3420 : Loss 3007.9510\n",
      "Iteration 3430 : Loss 3007.4873\n",
      "Iteration 3440 : Loss 3007.0278\n",
      "Iteration 3450 : Loss 3006.5724\n",
      "Iteration 3460 : Loss 3006.1212\n",
      "Iteration 3470 : Loss 3005.6739\n",
      "Iteration 3480 : Loss 3005.2307\n",
      "Iteration 3490 : Loss 3004.7915\n",
      "Iteration 3500 : Loss 3004.3562\n",
      "Iteration 3510 : Loss 3003.9247\n",
      "Iteration 3520 : Loss 3003.4972\n",
      "Iteration 3530 : Loss 3003.0735\n",
      "Iteration 3540 : Loss 3002.6536\n",
      "Iteration 3550 : Loss 3002.2374\n",
      "Iteration 3560 : Loss 3001.8249\n",
      "Iteration 3570 : Loss 3001.4161\n",
      "Iteration 3580 : Loss 3001.0110\n",
      "Iteration 3590 : Loss 3000.6095\n",
      "Iteration 3600 : Loss 3000.2115\n",
      "Iteration 3610 : Loss 2999.8171\n",
      "Iteration 3620 : Loss 2999.4262\n",
      "Iteration 3630 : Loss 2999.0387\n",
      "Iteration 3640 : Loss 2998.6547\n",
      "Iteration 3650 : Loss 2998.2741\n",
      "Iteration 3660 : Loss 2997.8969\n",
      "Iteration 3670 : Loss 2997.5231\n",
      "Iteration 3680 : Loss 2997.1525\n",
      "Iteration 3690 : Loss 2996.7852\n",
      "Iteration 3700 : Loss 2996.4212\n",
      "Iteration 3710 : Loss 2996.0603\n",
      "Iteration 3720 : Loss 2995.7027\n",
      "Iteration 3730 : Loss 2995.3482\n",
      "Iteration 3740 : Loss 2994.9969\n",
      "Iteration 3750 : Loss 2994.6486\n",
      "Iteration 3760 : Loss 2994.3034\n",
      "Iteration 3770 : Loss 2993.9612\n",
      "Iteration 3780 : Loss 2993.6221\n",
      "Iteration 3790 : Loss 2993.2859\n",
      "Iteration 3800 : Loss 2992.9527\n",
      "Iteration 3810 : Loss 2992.6224\n",
      "Iteration 3820 : Loss 2992.2950\n",
      "Iteration 3830 : Loss 2991.9704\n",
      "Iteration 3840 : Loss 2991.6487\n",
      "Iteration 3850 : Loss 2991.3299\n",
      "Iteration 3860 : Loss 2991.0138\n",
      "Iteration 3870 : Loss 2990.7004\n",
      "Iteration 3880 : Loss 2990.3898\n",
      "Iteration 3890 : Loss 2990.0819\n",
      "Iteration 3900 : Loss 2989.7767\n",
      "Iteration 3910 : Loss 2989.4742\n",
      "Iteration 3920 : Loss 2989.1743\n",
      "Iteration 3930 : Loss 2988.8770\n",
      "Iteration 3940 : Loss 2988.5822\n",
      "Iteration 3950 : Loss 2988.2901\n",
      "Iteration 3960 : Loss 2988.0004\n",
      "Iteration 3970 : Loss 2987.7133\n",
      "Iteration 3980 : Loss 2987.4287\n",
      "Iteration 3990 : Loss 2987.1465\n",
      "Iteration 4000 : Loss 2986.8668\n",
      "Iteration 4010 : Loss 2986.5895\n",
      "Iteration 4020 : Loss 2986.3146\n",
      "Iteration 4030 : Loss 2986.0421\n",
      "Iteration 4040 : Loss 2985.7719\n",
      "Iteration 4050 : Loss 2985.5041\n",
      "Iteration 4060 : Loss 2985.2385\n",
      "Iteration 4070 : Loss 2984.9753\n",
      "Iteration 4080 : Loss 2984.7143\n",
      "Iteration 4090 : Loss 2984.4555\n",
      "Iteration 4100 : Loss 2984.1990\n",
      "Iteration 4110 : Loss 2983.9447\n",
      "Iteration 4120 : Loss 2983.6926\n",
      "Iteration 4130 : Loss 2983.4426\n",
      "Iteration 4140 : Loss 2983.1947\n",
      "Iteration 4150 : Loss 2982.9490\n",
      "Iteration 4160 : Loss 2982.7054\n",
      "Iteration 4170 : Loss 2982.4639\n",
      "Iteration 4180 : Loss 2982.2244\n",
      "Iteration 4190 : Loss 2981.9870\n",
      "Iteration 4200 : Loss 2981.7516\n",
      "Iteration 4210 : Loss 2981.5182\n",
      "Iteration 4220 : Loss 2981.2868\n",
      "Iteration 4230 : Loss 2981.0574\n",
      "Iteration 4240 : Loss 2980.8299\n",
      "Iteration 4250 : Loss 2980.6043\n",
      "Iteration 4260 : Loss 2980.3807\n",
      "Iteration 4270 : Loss 2980.1590\n",
      "Iteration 4280 : Loss 2979.9391\n",
      "Iteration 4290 : Loss 2979.7211\n",
      "Iteration 4300 : Loss 2979.5050\n",
      "Iteration 4310 : Loss 2979.2906\n",
      "Iteration 4320 : Loss 2979.0781\n",
      "Iteration 4330 : Loss 2978.8674\n",
      "Iteration 4340 : Loss 2978.6585\n",
      "Iteration 4350 : Loss 2978.4513\n",
      "Iteration 4360 : Loss 2978.2459\n",
      "Iteration 4370 : Loss 2978.0422\n",
      "Iteration 4380 : Loss 2977.8402\n",
      "Iteration 4390 : Loss 2977.6399\n",
      "Iteration 4400 : Loss 2977.4413\n",
      "Iteration 4410 : Loss 2977.2444\n",
      "Iteration 4420 : Loss 2977.0491\n",
      "Iteration 4430 : Loss 2976.8555\n",
      "Iteration 4440 : Loss 2976.6635\n",
      "Iteration 4450 : Loss 2976.4730\n",
      "Iteration 4460 : Loss 2976.2842\n",
      "Iteration 4470 : Loss 2976.0970\n",
      "Iteration 4480 : Loss 2975.9113\n",
      "Iteration 4490 : Loss 2975.7272\n",
      "Iteration 4500 : Loss 2975.5446\n",
      "Iteration 4510 : Loss 2975.3635\n",
      "Iteration 4520 : Loss 2975.1839\n",
      "Iteration 4530 : Loss 2975.0059\n",
      "Iteration 4540 : Loss 2974.8293\n",
      "Iteration 4550 : Loss 2974.6542\n",
      "Iteration 4560 : Loss 2974.4805\n",
      "Iteration 4570 : Loss 2974.3083\n",
      "Iteration 4580 : Loss 2974.1375\n",
      "Iteration 4590 : Loss 2973.9681\n",
      "Iteration 4600 : Loss 2973.8001\n",
      "Iteration 4610 : Loss 2973.6335\n",
      "Iteration 4620 : Loss 2973.4683\n",
      "Iteration 4630 : Loss 2973.3044\n",
      "Iteration 4640 : Loss 2973.1419\n",
      "Iteration 4650 : Loss 2972.9807\n",
      "Iteration 4660 : Loss 2972.8209\n",
      "Iteration 4670 : Loss 2972.6624\n",
      "Iteration 4680 : Loss 2972.5052\n",
      "Iteration 4690 : Loss 2972.3492\n",
      "Iteration 4700 : Loss 2972.1946\n",
      "Iteration 4710 : Loss 2972.0412\n",
      "Iteration 4720 : Loss 2971.8891\n",
      "Iteration 4730 : Loss 2971.7382\n",
      "Iteration 4740 : Loss 2971.5886\n",
      "Iteration 4750 : Loss 2971.4402\n",
      "Iteration 4760 : Loss 2971.2930\n",
      "Iteration 4770 : Loss 2971.1470\n",
      "Iteration 4780 : Loss 2971.0022\n",
      "Iteration 4790 : Loss 2970.8585\n",
      "Iteration 4800 : Loss 2970.7161\n",
      "Iteration 4810 : Loss 2970.5748\n",
      "Iteration 4820 : Loss 2970.4346\n",
      "Iteration 4830 : Loss 2970.2956\n",
      "Iteration 4840 : Loss 2970.1577\n",
      "Iteration 4850 : Loss 2970.0210\n",
      "Iteration 4860 : Loss 2969.8853\n",
      "Iteration 4870 : Loss 2969.7507\n",
      "Iteration 4880 : Loss 2969.6173\n",
      "Iteration 4890 : Loss 2969.4849\n",
      "Iteration 4900 : Loss 2969.3536\n",
      "Iteration 4910 : Loss 2969.2233\n",
      "Iteration 4920 : Loss 2969.0941\n",
      "Iteration 4930 : Loss 2968.9659\n",
      "Iteration 4940 : Loss 2968.8388\n",
      "Iteration 4950 : Loss 2968.7127\n",
      "Iteration 4960 : Loss 2968.5876\n",
      "Iteration 4970 : Loss 2968.4635\n",
      "Iteration 4980 : Loss 2968.3404\n",
      "Iteration 4990 : Loss 2968.2183\n",
      "Iteration 5000 : Loss 2968.0972\n",
      "Iteration 5010 : Loss 2967.9770\n",
      "Iteration 5020 : Loss 2967.8578\n",
      "Iteration 5030 : Loss 2967.7396\n",
      "Iteration 5040 : Loss 2967.6223\n",
      "Iteration 5050 : Loss 2967.5059\n",
      "Iteration 5060 : Loss 2967.3905\n",
      "Iteration 5070 : Loss 2967.2759\n",
      "Iteration 5080 : Loss 2967.1623\n",
      "Iteration 5090 : Loss 2967.0496\n",
      "Iteration 5100 : Loss 2966.9378\n",
      "Iteration 5110 : Loss 2966.8269\n",
      "Iteration 5120 : Loss 2966.7169\n",
      "Iteration 5130 : Loss 2966.6077\n",
      "Iteration 5140 : Loss 2966.4994\n",
      "Iteration 5150 : Loss 2966.3919\n",
      "Iteration 5160 : Loss 2966.2853\n",
      "Iteration 5170 : Loss 2966.1796\n",
      "Iteration 5180 : Loss 2966.0747\n",
      "Iteration 5190 : Loss 2965.9706\n",
      "Iteration 5200 : Loss 2965.8673\n",
      "Iteration 5210 : Loss 2965.7649\n",
      "Iteration 5220 : Loss 2965.6632\n",
      "Iteration 5230 : Loss 2965.5623\n",
      "Iteration 5240 : Loss 2965.4623\n",
      "Iteration 5250 : Loss 2965.3630\n",
      "Iteration 5260 : Loss 2965.2645\n",
      "Iteration 5270 : Loss 2965.1668\n",
      "Iteration 5280 : Loss 2965.0698\n",
      "Iteration 5290 : Loss 2964.9736\n",
      "Iteration 5300 : Loss 2964.8782\n",
      "Iteration 5310 : Loss 2964.7835\n",
      "Iteration 5320 : Loss 2964.6895\n",
      "Iteration 5330 : Loss 2964.5963\n",
      "Iteration 5340 : Loss 2964.5037\n",
      "Iteration 5350 : Loss 2964.4120\n",
      "Iteration 5360 : Loss 2964.3209\n",
      "Iteration 5370 : Loss 2964.2305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5380 : Loss 2964.1408\n",
      "Iteration 5390 : Loss 2964.0519\n",
      "Iteration 5400 : Loss 2963.9636\n",
      "Iteration 5410 : Loss 2963.8760\n",
      "Iteration 5420 : Loss 2963.7891\n",
      "Iteration 5430 : Loss 2963.7028\n",
      "Iteration 5440 : Loss 2963.6172\n",
      "Iteration 5450 : Loss 2963.5323\n",
      "Iteration 5460 : Loss 2963.4480\n",
      "Iteration 5470 : Loss 2963.3644\n",
      "Iteration 5480 : Loss 2963.2814\n",
      "Iteration 5490 : Loss 2963.1991\n",
      "Iteration 5500 : Loss 2963.1173\n",
      "Iteration 5510 : Loss 2963.0363\n",
      "Iteration 5520 : Loss 2962.9558\n",
      "Iteration 5530 : Loss 2962.8759\n",
      "Iteration 5540 : Loss 2962.7967\n",
      "Iteration 5550 : Loss 2962.7181\n",
      "Iteration 5560 : Loss 2962.6400\n",
      "Iteration 5570 : Loss 2962.5626\n",
      "Iteration 5580 : Loss 2962.4858\n",
      "Iteration 5590 : Loss 2962.4095\n",
      "Iteration 5600 : Loss 2962.3338\n",
      "Iteration 5610 : Loss 2962.2587\n",
      "Iteration 5620 : Loss 2962.1842\n",
      "Iteration 5630 : Loss 2962.1102\n",
      "Iteration 5640 : Loss 2962.0368\n",
      "Iteration 5650 : Loss 2961.9639\n",
      "Iteration 5660 : Loss 2961.8916\n",
      "Iteration 5670 : Loss 2961.8199\n",
      "Iteration 5680 : Loss 2961.7486\n",
      "Iteration 5690 : Loss 2961.6780\n",
      "Iteration 5700 : Loss 2961.6078\n",
      "Iteration 5710 : Loss 2961.5382\n",
      "Iteration 5720 : Loss 2961.4691\n",
      "Iteration 5730 : Loss 2961.4005\n",
      "Iteration 5740 : Loss 2961.3325\n",
      "Iteration 5750 : Loss 2961.2649\n",
      "Iteration 5760 : Loss 2961.1979\n",
      "Iteration 5770 : Loss 2961.1313\n",
      "Iteration 5780 : Loss 2961.0653\n",
      "Iteration 5790 : Loss 2960.9997\n",
      "Iteration 5800 : Loss 2960.9347\n",
      "Iteration 5810 : Loss 2960.8701\n",
      "Iteration 5820 : Loss 2960.8060\n",
      "Iteration 5830 : Loss 2960.7424\n",
      "Iteration 5840 : Loss 2960.6792\n",
      "Iteration 5850 : Loss 2960.6165\n",
      "Iteration 5860 : Loss 2960.5543\n",
      "Iteration 5870 : Loss 2960.4926\n",
      "Iteration 5880 : Loss 2960.4313\n",
      "Iteration 5890 : Loss 2960.3705\n",
      "Iteration 5900 : Loss 2960.3101\n",
      "Iteration 5910 : Loss 2960.2501\n",
      "Iteration 5920 : Loss 2960.1906\n",
      "Iteration 5930 : Loss 2960.1316\n",
      "Iteration 5940 : Loss 2960.0729\n",
      "Iteration 5950 : Loss 2960.0147\n",
      "Iteration 5960 : Loss 2959.9570\n",
      "Iteration 5970 : Loss 2959.8996\n",
      "Iteration 5980 : Loss 2959.8427\n",
      "Iteration 5990 : Loss 2959.7862\n",
      "Iteration 6000 : Loss 2959.7301\n",
      "Iteration 6010 : Loss 2959.6744\n",
      "Iteration 6020 : Loss 2959.6191\n",
      "Iteration 6030 : Loss 2959.5642\n",
      "Iteration 6040 : Loss 2959.5098\n",
      "Iteration 6050 : Loss 2959.4557\n",
      "Iteration 6060 : Loss 2959.4020\n",
      "Iteration 6070 : Loss 2959.3487\n",
      "Iteration 6080 : Loss 2959.2958\n",
      "Iteration 6090 : Loss 2959.2433\n",
      "Iteration 6100 : Loss 2959.1911\n",
      "Iteration 6110 : Loss 2959.1394\n",
      "Iteration 6120 : Loss 2959.0880\n",
      "Iteration 6130 : Loss 2959.0370\n",
      "Iteration 6140 : Loss 2958.9863\n",
      "Iteration 6150 : Loss 2958.9360\n",
      "Iteration 6160 : Loss 2958.8861\n",
      "Iteration 6170 : Loss 2958.8366\n",
      "Iteration 6180 : Loss 2958.7873\n",
      "Iteration 6190 : Loss 2958.7385\n",
      "Iteration 6200 : Loss 2958.6900\n",
      "Iteration 6210 : Loss 2958.6418\n",
      "Iteration 6220 : Loss 2958.5940\n",
      "Iteration 6230 : Loss 2958.5465\n",
      "Iteration 6240 : Loss 2958.4994\n",
      "Iteration 6250 : Loss 2958.4526\n",
      "Iteration 6260 : Loss 2958.4062\n",
      "Iteration 6270 : Loss 2958.3600\n",
      "Iteration 6280 : Loss 2958.3142\n",
      "Iteration 6290 : Loss 2958.2687\n",
      "Iteration 6300 : Loss 2958.2236\n",
      "Iteration 6310 : Loss 2958.1788\n",
      "Iteration 6320 : Loss 2958.1342\n",
      "Iteration 6330 : Loss 2958.0900\n",
      "Iteration 6340 : Loss 2958.0462\n",
      "Iteration 6350 : Loss 2958.0026\n",
      "Iteration 6360 : Loss 2957.9593\n",
      "Iteration 6370 : Loss 2957.9163\n",
      "Iteration 6380 : Loss 2957.8737\n",
      "Iteration 6390 : Loss 2957.8313\n",
      "Iteration 6400 : Loss 2957.7892\n",
      "Iteration 6410 : Loss 2957.7475\n",
      "Iteration 6420 : Loss 2957.7060\n",
      "Iteration 6430 : Loss 2957.6648\n",
      "Iteration 6440 : Loss 2957.6239\n",
      "Iteration 6450 : Loss 2957.5833\n",
      "Iteration 6460 : Loss 2957.5430\n",
      "Iteration 6470 : Loss 2957.5029\n",
      "Iteration 6480 : Loss 2957.4631\n",
      "Iteration 6490 : Loss 2957.4236\n",
      "Iteration 6500 : Loss 2957.3844\n",
      "Iteration 6510 : Loss 2957.3455\n",
      "Iteration 6520 : Loss 2957.3068\n",
      "Iteration 6530 : Loss 2957.2684\n",
      "Iteration 6540 : Loss 2957.2302\n",
      "Iteration 6550 : Loss 2957.1923\n",
      "Iteration 6560 : Loss 2957.1547\n",
      "Iteration 6570 : Loss 2957.1174\n",
      "Iteration 6580 : Loss 2957.0803\n",
      "Iteration 6590 : Loss 2957.0434\n",
      "Iteration 6600 : Loss 2957.0068\n",
      "Iteration 6610 : Loss 2956.9705\n",
      "Iteration 6620 : Loss 2956.9344\n",
      "Iteration 6630 : Loss 2956.8985\n",
      "Iteration 6640 : Loss 2956.8629\n",
      "Iteration 6650 : Loss 2956.8276\n",
      "Iteration 6660 : Loss 2956.7924\n",
      "Iteration 6670 : Loss 2956.7576\n",
      "Iteration 6680 : Loss 2956.7229\n",
      "Iteration 6690 : Loss 2956.6885\n",
      "Iteration 6700 : Loss 2956.6543\n",
      "Iteration 6710 : Loss 2956.6204\n",
      "Iteration 6720 : Loss 2956.5867\n",
      "Iteration 6730 : Loss 2956.5532\n",
      "Iteration 6740 : Loss 2956.5199\n",
      "Iteration 6750 : Loss 2956.4869\n",
      "Iteration 6760 : Loss 2956.4541\n",
      "Iteration 6770 : Loss 2956.4215\n",
      "Iteration 6780 : Loss 2956.3891\n",
      "Iteration 6790 : Loss 2956.3570\n",
      "Iteration 6800 : Loss 2956.3250\n",
      "Iteration 6810 : Loss 2956.2933\n",
      "Iteration 6820 : Loss 2956.2618\n",
      "Iteration 6830 : Loss 2956.2305\n",
      "Iteration 6840 : Loss 2956.1994\n",
      "Iteration 6850 : Loss 2956.1685\n",
      "Iteration 6860 : Loss 2956.1379\n",
      "Iteration 6870 : Loss 2956.1074\n",
      "Iteration 6880 : Loss 2956.0771\n",
      "Iteration 6890 : Loss 2956.0470\n",
      "Iteration 6900 : Loss 2956.0172\n",
      "Iteration 6910 : Loss 2955.9875\n",
      "Iteration 6920 : Loss 2955.9580\n",
      "Iteration 6930 : Loss 2955.9287\n",
      "Iteration 6940 : Loss 2955.8996\n",
      "Iteration 6950 : Loss 2955.8708\n",
      "Iteration 6960 : Loss 2955.8420\n",
      "Iteration 6970 : Loss 2955.8135\n",
      "Iteration 6980 : Loss 2955.7852\n",
      "Iteration 6990 : Loss 2955.7571\n",
      "Iteration 7000 : Loss 2955.7291\n",
      "Iteration 7010 : Loss 2955.7013\n",
      "Iteration 7020 : Loss 2955.6737\n",
      "Iteration 7030 : Loss 2955.6463\n",
      "Iteration 7040 : Loss 2955.6191\n",
      "Iteration 7050 : Loss 2955.5920\n",
      "Iteration 7060 : Loss 2955.5651\n",
      "Iteration 7070 : Loss 2955.5384\n",
      "Iteration 7080 : Loss 2955.5119\n",
      "Iteration 7090 : Loss 2955.4855\n",
      "Iteration 7100 : Loss 2955.4593\n",
      "Iteration 7110 : Loss 2955.4333\n",
      "Iteration 7120 : Loss 2955.4074\n",
      "Iteration 7130 : Loss 2955.3817\n",
      "Iteration 7140 : Loss 2955.3562\n",
      "Iteration 7150 : Loss 2955.3308\n",
      "Iteration 7160 : Loss 2955.3056\n",
      "Iteration 7170 : Loss 2955.2806\n",
      "Iteration 7180 : Loss 2955.2557\n",
      "Iteration 7190 : Loss 2955.2310\n",
      "Iteration 7200 : Loss 2955.2065\n",
      "Iteration 7210 : Loss 2955.1820\n",
      "Iteration 7220 : Loss 2955.1578\n",
      "Iteration 7230 : Loss 2955.1337\n",
      "Iteration 7240 : Loss 2955.1098\n",
      "Iteration 7250 : Loss 2955.0860\n",
      "Iteration 7260 : Loss 2955.0623\n",
      "Iteration 7270 : Loss 2955.0388\n",
      "Iteration 7280 : Loss 2955.0155\n",
      "Iteration 7290 : Loss 2954.9923\n",
      "Iteration 7300 : Loss 2954.9693\n",
      "Iteration 7310 : Loss 2954.9464\n",
      "Iteration 7320 : Loss 2954.9236\n",
      "Iteration 7330 : Loss 2954.9010\n",
      "Iteration 7340 : Loss 2954.8785\n",
      "Iteration 7350 : Loss 2954.8562\n",
      "Iteration 7360 : Loss 2954.8340\n",
      "Iteration 7370 : Loss 2954.8119\n",
      "Iteration 7380 : Loss 2954.7900\n",
      "Iteration 7390 : Loss 2954.7682\n",
      "Iteration 7400 : Loss 2954.7466\n",
      "Iteration 7410 : Loss 2954.7251\n",
      "Iteration 7420 : Loss 2954.7037\n",
      "Iteration 7430 : Loss 2954.6824\n",
      "Iteration 7440 : Loss 2954.6613\n",
      "Iteration 7450 : Loss 2954.6403\n",
      "Iteration 7460 : Loss 2954.6195\n",
      "Iteration 7470 : Loss 2954.5988\n",
      "Iteration 7480 : Loss 2954.5782\n",
      "Iteration 7490 : Loss 2954.5577\n",
      "Iteration 7500 : Loss 2954.5373\n",
      "Iteration 7510 : Loss 2954.5171\n",
      "Iteration 7520 : Loss 2954.4970\n",
      "Iteration 7530 : Loss 2954.4770\n",
      "Iteration 7540 : Loss 2954.4572\n",
      "Iteration 7550 : Loss 2954.4374\n",
      "Iteration 7560 : Loss 2954.4178\n",
      "Iteration 7570 : Loss 2954.3983\n",
      "Iteration 7580 : Loss 2954.3789\n",
      "Iteration 7590 : Loss 2954.3597\n",
      "Iteration 7600 : Loss 2954.3405\n",
      "Iteration 7610 : Loss 2954.3215\n",
      "Iteration 7620 : Loss 2954.3026\n",
      "Iteration 7630 : Loss 2954.2838\n",
      "Iteration 7640 : Loss 2954.2651\n",
      "Iteration 7650 : Loss 2954.2465\n",
      "Iteration 7660 : Loss 2954.2281\n",
      "Iteration 7670 : Loss 2954.2097\n",
      "Iteration 7680 : Loss 2954.1915\n",
      "Iteration 7690 : Loss 2954.1733\n",
      "Iteration 7700 : Loss 2954.1553\n",
      "Iteration 7710 : Loss 2954.1374\n",
      "Iteration 7720 : Loss 2954.1196\n",
      "Iteration 7730 : Loss 2954.1019\n",
      "Iteration 7740 : Loss 2954.0842\n",
      "Iteration 7750 : Loss 2954.0667\n",
      "Iteration 7760 : Loss 2954.0493\n",
      "Iteration 7770 : Loss 2954.0321\n",
      "Iteration 7780 : Loss 2954.0149\n",
      "Iteration 7790 : Loss 2953.9978\n",
      "Iteration 7800 : Loss 2953.9808\n",
      "Iteration 7810 : Loss 2953.9639\n",
      "Iteration 7820 : Loss 2953.9471\n",
      "Iteration 7830 : Loss 2953.9304\n",
      "Iteration 7840 : Loss 2953.9138\n",
      "Iteration 7850 : Loss 2953.8973\n",
      "Iteration 7860 : Loss 2953.8809\n",
      "Iteration 7870 : Loss 2953.8646\n",
      "Iteration 7880 : Loss 2953.8484\n",
      "Iteration 7890 : Loss 2953.8322\n",
      "Iteration 7900 : Loss 2953.8162\n",
      "Iteration 7910 : Loss 2953.8003\n",
      "Iteration 7920 : Loss 2953.7844\n",
      "Iteration 7930 : Loss 2953.7687\n",
      "Iteration 7940 : Loss 2953.7530\n",
      "Iteration 7950 : Loss 2953.7374\n",
      "Iteration 7960 : Loss 2953.7219\n",
      "Iteration 7970 : Loss 2953.7065\n",
      "Iteration 7980 : Loss 2953.6912\n",
      "Iteration 7990 : Loss 2953.6760\n",
      "Iteration 8000 : Loss 2953.6609\n",
      "Iteration 8010 : Loss 2953.6458\n",
      "Iteration 8020 : Loss 2953.6308\n",
      "Iteration 8030 : Loss 2953.6160\n",
      "Iteration 8040 : Loss 2953.6012\n",
      "Iteration 8050 : Loss 2953.5864\n",
      "Iteration 8060 : Loss 2953.5718\n",
      "Iteration 8070 : Loss 2953.5573\n",
      "Iteration 8080 : Loss 2953.5428\n",
      "Iteration 8090 : Loss 2953.5284\n",
      "Iteration 8100 : Loss 2953.5141\n",
      "Iteration 8110 : Loss 2953.4999\n",
      "Iteration 8120 : Loss 2953.4857\n",
      "Iteration 8130 : Loss 2953.4717\n",
      "Iteration 8140 : Loss 2953.4577\n",
      "Iteration 8150 : Loss 2953.4438\n",
      "Iteration 8160 : Loss 2953.4299\n",
      "Iteration 8170 : Loss 2953.4162\n",
      "Iteration 8180 : Loss 2953.4025\n",
      "Iteration 8190 : Loss 2953.3889\n",
      "Iteration 8200 : Loss 2953.3753\n",
      "Iteration 8210 : Loss 2953.3619\n",
      "Iteration 8220 : Loss 2953.3485\n",
      "Iteration 8230 : Loss 2953.3352\n",
      "Iteration 8240 : Loss 2953.3219\n",
      "Iteration 8250 : Loss 2953.3088\n",
      "Iteration 8260 : Loss 2953.2957\n",
      "Iteration 8270 : Loss 2953.2826\n",
      "Iteration 8280 : Loss 2953.2697\n",
      "Iteration 8290 : Loss 2953.2568\n",
      "Iteration 8300 : Loss 2953.2440\n",
      "Iteration 8310 : Loss 2953.2312\n",
      "Iteration 8320 : Loss 2953.2186\n",
      "Iteration 8330 : Loss 2953.2059\n",
      "Iteration 8340 : Loss 2953.1934\n",
      "Iteration 8350 : Loss 2953.1809\n",
      "Iteration 8360 : Loss 2953.1685\n",
      "Iteration 8370 : Loss 2953.1562\n",
      "Iteration 8380 : Loss 2953.1439\n",
      "Iteration 8390 : Loss 2953.1317\n",
      "Iteration 8400 : Loss 2953.1195\n",
      "Iteration 8410 : Loss 2953.1074\n",
      "Iteration 8420 : Loss 2953.0954\n",
      "Iteration 8430 : Loss 2953.0835\n",
      "Iteration 8440 : Loss 2953.0716\n",
      "Iteration 8450 : Loss 2953.0597\n",
      "Iteration 8460 : Loss 2953.0480\n",
      "Iteration 8470 : Loss 2953.0363\n",
      "Iteration 8480 : Loss 2953.0246\n",
      "Iteration 8490 : Loss 2953.0130\n",
      "Iteration 8500 : Loss 2953.0015\n",
      "Iteration 8510 : Loss 2952.9900\n",
      "Iteration 8520 : Loss 2952.9786\n",
      "Iteration 8530 : Loss 2952.9673\n",
      "Iteration 8540 : Loss 2952.9560\n",
      "Iteration 8550 : Loss 2952.9448\n",
      "Iteration 8560 : Loss 2952.9336\n",
      "Iteration 8570 : Loss 2952.9225\n",
      "Iteration 8580 : Loss 2952.9114\n",
      "Iteration 8590 : Loss 2952.9004\n",
      "Iteration 8600 : Loss 2952.8895\n",
      "Iteration 8610 : Loss 2952.8786\n",
      "Iteration 8620 : Loss 2952.8678\n",
      "Iteration 8630 : Loss 2952.8570\n",
      "Iteration 8640 : Loss 2952.8462\n",
      "Iteration 8650 : Loss 2952.8356\n",
      "Iteration 8660 : Loss 2952.8250\n",
      "Iteration 8670 : Loss 2952.8144\n",
      "Iteration 8680 : Loss 2952.8039\n",
      "Iteration 8690 : Loss 2952.7934\n",
      "Iteration 8700 : Loss 2952.7830\n",
      "Iteration 8710 : Loss 2952.7727\n",
      "Iteration 8720 : Loss 2952.7624\n",
      "Iteration 8730 : Loss 2952.7521\n",
      "Iteration 8740 : Loss 2952.7419\n",
      "Iteration 8750 : Loss 2952.7318\n",
      "Iteration 8760 : Loss 2952.7217\n",
      "Iteration 8770 : Loss 2952.7116\n",
      "Iteration 8780 : Loss 2952.7016\n",
      "Iteration 8790 : Loss 2952.6917\n",
      "Iteration 8800 : Loss 2952.6818\n",
      "Iteration 8810 : Loss 2952.6719\n",
      "Iteration 8820 : Loss 2952.6621\n",
      "Iteration 8830 : Loss 2952.6523\n",
      "Iteration 8840 : Loss 2952.6426\n",
      "Iteration 8850 : Loss 2952.6330\n",
      "Iteration 8860 : Loss 2952.6234\n",
      "Iteration 8870 : Loss 2952.6138\n",
      "Iteration 8880 : Loss 2952.6043\n",
      "Iteration 8890 : Loss 2952.5948\n",
      "Iteration 8900 : Loss 2952.5854\n",
      "Iteration 8910 : Loss 2952.5760\n",
      "Iteration 8920 : Loss 2952.5666\n",
      "Iteration 8930 : Loss 2952.5573\n",
      "Iteration 8940 : Loss 2952.5481\n",
      "Iteration 8950 : Loss 2952.5389\n",
      "Iteration 8960 : Loss 2952.5297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8970 : Loss 2952.5206\n",
      "Iteration 8980 : Loss 2952.5115\n",
      "Iteration 8990 : Loss 2952.5025\n",
      "Iteration 9000 : Loss 2952.4935\n",
      "Iteration 9010 : Loss 2952.4845\n",
      "Iteration 9020 : Loss 2952.4756\n",
      "Iteration 9030 : Loss 2952.4667\n",
      "Iteration 9040 : Loss 2952.4579\n",
      "Iteration 9050 : Loss 2952.4491\n",
      "Iteration 9060 : Loss 2952.4404\n",
      "Iteration 9070 : Loss 2952.4317\n",
      "Iteration 9080 : Loss 2952.4230\n",
      "Iteration 9090 : Loss 2952.4144\n",
      "Iteration 9100 : Loss 2952.4058\n",
      "Iteration 9110 : Loss 2952.3973\n",
      "Iteration 9120 : Loss 2952.3888\n",
      "Iteration 9130 : Loss 2952.3803\n",
      "Iteration 9140 : Loss 2952.3719\n",
      "Iteration 9150 : Loss 2952.3635\n",
      "Iteration 9160 : Loss 2952.3551\n",
      "Iteration 9170 : Loss 2952.3468\n",
      "Iteration 9180 : Loss 2952.3385\n",
      "Iteration 9190 : Loss 2952.3303\n",
      "Iteration 9200 : Loss 2952.3221\n",
      "Iteration 9210 : Loss 2952.3139\n",
      "Iteration 9220 : Loss 2952.3058\n",
      "Iteration 9230 : Loss 2952.2977\n",
      "Iteration 9240 : Loss 2952.2896\n",
      "Iteration 9250 : Loss 2952.2816\n",
      "Iteration 9260 : Loss 2952.2736\n",
      "Iteration 9270 : Loss 2952.2657\n",
      "Iteration 9280 : Loss 2952.2578\n",
      "Iteration 9290 : Loss 2952.2499\n",
      "Iteration 9300 : Loss 2952.2420\n",
      "Iteration 9310 : Loss 2952.2342\n",
      "Iteration 9320 : Loss 2952.2265\n",
      "Iteration 9330 : Loss 2952.2187\n",
      "Iteration 9340 : Loss 2952.2110\n",
      "Iteration 9350 : Loss 2952.2033\n",
      "Iteration 9360 : Loss 2952.1957\n",
      "Iteration 9370 : Loss 2952.1881\n",
      "Iteration 9380 : Loss 2952.1805\n",
      "Iteration 9390 : Loss 2952.1730\n",
      "Iteration 9400 : Loss 2952.1655\n",
      "Iteration 9410 : Loss 2952.1580\n",
      "Iteration 9420 : Loss 2952.1505\n",
      "Iteration 9430 : Loss 2952.1431\n",
      "Iteration 9440 : Loss 2952.1357\n",
      "Iteration 9450 : Loss 2952.1284\n",
      "Iteration 9460 : Loss 2952.1211\n",
      "Iteration 9470 : Loss 2952.1138\n",
      "Iteration 9480 : Loss 2952.1065\n",
      "Iteration 9490 : Loss 2952.0993\n",
      "Iteration 9500 : Loss 2952.0921\n",
      "Iteration 9510 : Loss 2952.0849\n",
      "Iteration 9520 : Loss 2952.0778\n",
      "Iteration 9530 : Loss 2952.0707\n",
      "Iteration 9540 : Loss 2952.0636\n",
      "Iteration 9550 : Loss 2952.0565\n",
      "Iteration 9560 : Loss 2952.0495\n",
      "Iteration 9570 : Loss 2952.0425\n",
      "Iteration 9580 : Loss 2952.0355\n",
      "Iteration 9590 : Loss 2952.0286\n",
      "Iteration 9600 : Loss 2952.0217\n",
      "Iteration 9610 : Loss 2952.0148\n",
      "Iteration 9620 : Loss 2952.0080\n",
      "Iteration 9630 : Loss 2952.0012\n",
      "Iteration 9640 : Loss 2951.9944\n",
      "Iteration 9650 : Loss 2951.9876\n",
      "Iteration 9660 : Loss 2951.9809\n",
      "Iteration 9670 : Loss 2951.9741\n",
      "Iteration 9680 : Loss 2951.9674\n",
      "Iteration 9690 : Loss 2951.9608\n",
      "Iteration 9700 : Loss 2951.9542\n",
      "Iteration 9710 : Loss 2951.9475\n",
      "Iteration 9720 : Loss 2951.9410\n",
      "Iteration 9730 : Loss 2951.9344\n",
      "Iteration 9740 : Loss 2951.9279\n",
      "Iteration 9750 : Loss 2951.9214\n",
      "Iteration 9760 : Loss 2951.9149\n",
      "Iteration 9770 : Loss 2951.9084\n",
      "Iteration 9780 : Loss 2951.9020\n",
      "Iteration 9790 : Loss 2951.8956\n",
      "Iteration 9800 : Loss 2951.8892\n",
      "Iteration 9810 : Loss 2951.8829\n",
      "Iteration 9820 : Loss 2951.8766\n",
      "Iteration 9830 : Loss 2951.8703\n",
      "Iteration 9840 : Loss 2951.8640\n",
      "Iteration 9850 : Loss 2951.8577\n",
      "Iteration 9860 : Loss 2951.8515\n",
      "Iteration 9870 : Loss 2951.8453\n",
      "Iteration 9880 : Loss 2951.8391\n",
      "Iteration 9890 : Loss 2951.8329\n",
      "Iteration 9900 : Loss 2951.8268\n",
      "Iteration 9910 : Loss 2951.8207\n",
      "Iteration 9920 : Loss 2951.8146\n",
      "Iteration 9930 : Loss 2951.8085\n",
      "Iteration 9940 : Loss 2951.8025\n",
      "Iteration 9950 : Loss 2951.7964\n",
      "Iteration 9960 : Loss 2951.7904\n",
      "Iteration 9970 : Loss 2951.7845\n",
      "Iteration 9980 : Loss 2951.7785\n",
      "Iteration 9990 : Loss 2951.7726\n",
      "Iteration 10000 : Loss 2951.7667\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c76835",
   "metadata": {},
   "source": [
    "## (10) test 데이터에 대한 성능 확인하기\n",
    "\n",
    "test 데이터에 대한 성능을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12f383ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2824.45926980078"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9059ec7e",
   "metadata": {},
   "source": [
    "## (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "\n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b65239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044fe7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
